{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY7ekQ4L_C4x"
   },
   "source": [
    "**DATA FOLDING AND SPLITTING**\n",
    "\n",
    "**Data Source:** Reads Twitter data from a specified file containing tweet information, separating tweets into their types (e.g., non-rumor, news, false rumor, true rumor, unverified) and respective IDs.\n",
    "\n",
    "**Train-Test Split:** Divides the tweet data into train and test sets using a folding technique.\n",
    "\n",
    "**Balanced Representation:** Ensures an equitable distribution of different tweet types in both training and testing sets across multiple folds.\n",
    "\n",
    "**Output Details:** Provides the count and specific tweet IDs for each fold's training and testing datasets, aiding in comprehensive analysis and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_q2ixhe0jWrx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.strip().split('\\t')\n",
    "            tweet_type = values[0]\n",
    "            tweet_id = values[2]\n",
    "            data.append((tweet_type, tweet_id))\n",
    "    return data\n",
    "\n",
    "def split_data_by_type(data):\n",
    "    data_by_type = {}\n",
    "    for tweet_type, tweet_id in data:\n",
    "        if tweet_type not in data_by_type:\n",
    "            data_by_type[tweet_type] = []\n",
    "        data_by_type[tweet_type].append(tweet_id)\n",
    "    return data_by_type\n",
    "\n",
    "def split_train_test_data(data_by_type, folds=5):\n",
    "    train_test_splits = []\n",
    "    for _ in range(folds):\n",
    "        train_set = []\n",
    "        test_set = []\n",
    "        for tweet_type, tweets in data_by_type.items():\n",
    "            random.shuffle(tweets)\n",
    "            split_index = len(tweets) // folds\n",
    "            test_split = tweets[:split_index]\n",
    "            train_split = tweets[split_index:]\n",
    "            train_set.extend(train_split)\n",
    "            test_set.extend(test_split)\n",
    "        random.shuffle(train_set)\n",
    "        random.shuffle(test_set)\n",
    "        train_test_splits.append((train_set, test_set))\n",
    "    return train_test_splits\n",
    "\n",
    "# File path to your data file\n",
    "file_path = '/content/drive/MyDrive/ProjectData/Twitter15_label_All.txt'\n",
    "\n",
    "# Load data from file\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Split data by tweet type\n",
    "data_by_type = split_data_by_type(data)\n",
    "\n",
    "# Split data into train and test for each fold\n",
    "folds_count = 5  # Modify as needed\n",
    "train_test_splits = split_train_test_data(data_by_type, folds=folds_count)\n",
    "\n",
    "# Print train and test sets for each fold\n",
    "for i, (train_set, test_set) in enumerate(train_test_splits):\n",
    "    print(f\"Fold {i + 1} - Train Data:\", len(train_set))\n",
    "    print(f\"Fold {i + 1} - Test Data:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byA7Dax09lnD"
   },
   "outputs": [],
   "source": [
    "for i, (train_set, test_set) in enumerate(train_test_splits):\n",
    "    print(f\"Fold {i + 1} - Train Data:\")\n",
    "    print(train_set)\n",
    "    print(f\"Fold {i + 1} - Test Data:\")\n",
    "    print(test_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlkhXdmK-dHz"
   },
   "outputs": [],
   "source": [
    "def read_twitter_tree(file_path):\n",
    "    print(\"Reading Twitter tree...\")\n",
    "    tree_data = {}\n",
    "\n",
    "    file = open(file_path, 'r')\n",
    "    for line in file:\n",
    "        eid, index_p, index_c, max_degree, max_l, vec = line.rstrip().split('\\t')[:6]\n",
    "        index_c = int(index_c)\n",
    "        max_degree, max_l = int(max_degree), int(max_l)\n",
    "\n",
    "        tree_data.setdefault(eid, {})[index_c] = {'parent': index_p, 'max_degree': max_degree, 'maxL': max_l, 'vec': vec}\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    print(f\"Number of trees loaded: {len(tree_data)}\")\n",
    "    return tree_data\n",
    "\n",
    "# Provide the path to your file\n",
    "file_path = '/content/drive/MyDrive/ProjectData/data.TD_RvNN.vol_5000.txt'\n",
    "\n",
    "# Call the function with the file path\n",
    "tree_data = read_twitter_tree(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SK86oSiKlFoM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from dateutil import rrule\n",
    "\n",
    "def get_months(create_at):\n",
    "    create_at = create_at.split(' ')[0]  # Extracting only the date part\n",
    "    date_i = datetime.datetime.strptime(create_at, '%Y-%m-%d').date()  # Converting to date object\n",
    "\n",
    "    # Define the current date\n",
    "    today = datetime.datetime.now().date()\n",
    "\n",
    "    # Calculate the number of months between the account creation date and the current date\n",
    "    month_sep = rrule.rrule(rrule.MONTHLY, dtstart=date_i, until=today)\n",
    "    return month_sep.count()\n",
    "\n",
    "def get_edge_index(length):\n",
    "    edge_index = []\n",
    "    for i in range(length):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        edge_index.append([0, i])\n",
    "        edge_index.append([i, 0])\n",
    "    return edge_index\n",
    "\n",
    "def extract_user_features(data):\n",
    "    user_features = []\n",
    "    for index, row in data.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        user_status = row['user_status']\n",
    "        if user_status == 0:  # Non-existent accounts\n",
    "            features = [user_id] + [-1] * 9 + [user_status]\n",
    "        else:\n",
    "            features = [row['user_id'], row['url'], row['protected'], row['verified'],\n",
    "                        row['followers_count'], row['friends_count'],\n",
    "                        row['listed_count'], row['favourites_count'],\n",
    "                        row['statuses_count'], get_months(row['created_at']),\n",
    "                        row['user_status']]\n",
    "        user_features.append(features)\n",
    "    return user_features\n",
    "\n",
    "def extract_friend_features(data):\n",
    "    friend_features = []\n",
    "    friends_list = data[data['reason'].isna()]['friend_id'].unique().tolist()\n",
    "    for friend_id in friends_list:\n",
    "        friend_data = data[data['friend_id'] == friend_id].iloc[0, :]\n",
    "        friend_status = friend_data['user_status']\n",
    "        if friend_status == 0:  # Non-existent accounts\n",
    "            features = [friend_id] + [-1] * 9 + [friend_status]\n",
    "        else:\n",
    "            features = [friend_data['friend_id'], friend_data['url'], friend_data['protected'], friend_data['verified'],\n",
    "                        friend_data['followers_count'], friend_data['friends_count'],\n",
    "                        friend_data['listed_count'], friend_data['favourites_count'],\n",
    "                        friend_data['statuses_count'], get_months(friend_data['created_at']),\n",
    "                        friend_data['user_status']]\n",
    "        friend_features.append(features)\n",
    "    return friend_features\n",
    "\n",
    "def normalize_features(data):\n",
    "    cols = ['url', 'protected', 'verified', 'followers_count', 'friends_count',\n",
    "            'listed_count', 'favourites_count', 'statuses_count', 'created_at', 'user_status']\n",
    "\n",
    "    for col in cols:\n",
    "        mean = np.mean(data[col])\n",
    "        std = np.std(data[col])\n",
    "        if std:\n",
    "            data[col] = data[col].apply(lambda x: (x - mean) / std)\n",
    "    return data\n",
    "\n",
    "def main(obj):\n",
    "    user_info = pd.read_csv('/content/drive/MyDrive/ProjectData/' + obj + '_User_Information.csv', sep='\\t')\n",
    "    user_friends = pd.read_csv('/content/drive/MyDrive/ProjectData/' + obj + '_User_Friends.csv', sep='\\t')\n",
    "    ego_relation = pd.read_csv('/content/drive/MyDrive/ProjectData/' + obj + '_Ego_Relationships.csv', sep='\\t')\n",
    "\n",
    "    user_features = extract_user_features(user_info)\n",
    "    friend_features = extract_friend_features(user_friends)\n",
    "\n",
    "    total_feature = user_features + friend_features\n",
    "    data_total = pd.DataFrame(total_feature,\n",
    "                              columns=['user_id', 'url', 'protected', 'verified', 'followers_count', 'friends_count',\n",
    "                                       'listed_count', 'favourites_count', 'statuses_count',\n",
    "                                       'created_at', 'user_status'])\n",
    "\n",
    "    data_total = normalize_features(data_total)\n",
    "    # Initialize a list to store dictionaries containing features\n",
    "    standard_list = []\n",
    "\n",
    "    # Assuming data_3['user_id'] is used to create user_id_friend\n",
    "    user_id_friend = ego_relation['user_id'].unique().tolist()\n",
    "\n",
    "    for index, row in user_info.iterrows():\n",
    "        twitter_id = row['twitter_id']\n",
    "        user_id = row['user_id']\n",
    "        tree_feature = []\n",
    "\n",
    "        # Construct root feature\n",
    "        user_data = data_total[data_total['user_id'] == user_id].iloc[0]\n",
    "        root_feature = [\n",
    "            float(user_data['url']), float(user_data['protected']), float(user_data['verified']),\n",
    "            float(user_data['followers_count']), float(user_data['friends_count']),\n",
    "            float(user_data['listed_count']), float(user_data['favourites_count']),\n",
    "            float(user_data['statuses_count']), float(user_data['created_at']),\n",
    "            float(user_data['user_status'])\n",
    "        ]\n",
    "        tree_feature.append(root_feature)\n",
    "\n",
    "        # Checking if user_id is in user_id_friend\n",
    "        if user_id in user_id_friend:\n",
    "            friend_ids = user_friends[(user_friends['user_id'] == user_id) & (user_friends['user_status'] == 1)]['friend_id'].tolist()\n",
    "            for friend_id in friend_ids:\n",
    "                friend_data = data_total[data_total['user_id'] == friend_id].iloc[0]\n",
    "                temp_feature = [\n",
    "                    float(friend_data['url']), float(friend_data['protected']), float(friend_data['verified']),\n",
    "                    float(friend_data['followers_count']), float(friend_data['friends_count']),\n",
    "                    float(friend_data['listed_count']), float(friend_data['favourites_count']),\n",
    "                    float(friend_data['statuses_count']), float(friend_data['created_at']),\n",
    "                    float(friend_data['user_status'])\n",
    "                ]\n",
    "                tree_feature.append(temp_feature)\n",
    "\n",
    "            # Create dictionary to store features for this user\n",
    "            temp_dict = {\n",
    "                'twitter_id': twitter_id,\n",
    "                'user_id': str(user_id),\n",
    "                'root_feature': np.array(root_feature),\n",
    "                'tree_feature': np.array(tree_feature),\n",
    "                'edge_index': np.array(get_edge_index(len(tree_feature))),\n",
    "                'root_index': 0,\n",
    "                'tree_len': len(tree_feature),\n",
    "                'status': True\n",
    "            }\n",
    "            standard_list.append(temp_dict)\n",
    "        else:\n",
    "            # User_id not found in user_id_friend\n",
    "            temp_dict = {\n",
    "                'twitter_id': twitter_id,\n",
    "                'user_id': str(user_id),\n",
    "                'root_feature': np.array(root_feature),\n",
    "                'tree_feature': np.array(tree_feature),\n",
    "                'edge_index': np.array(get_edge_index(len(tree_feature))),\n",
    "                'root_index': 0,\n",
    "                'tree_len': len(tree_feature),\n",
    "                'status': False\n",
    "            }\n",
    "            standard_list.append(temp_dict)\n",
    "    return standard_list\n",
    "\n",
    "total_graph = main('Twitter15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b6W405CEiwZ"
   },
   "outputs": [],
   "source": [
    " for i, dic_sample in enumerate(total_graph):\n",
    "            file_path = f'/content/drive/MyDrive/ProjectData/Ego_graph/{dic_sample[\"twitter_id\"]}.npz'  # Replace with your desired folder path\n",
    "            np.savez(file_path,\n",
    "                    user_id=dic_sample['user_id'],\n",
    "                    root_feature=dic_sample['root_feature'],\n",
    "                    tree_feature=dic_sample['tree_feature'],\n",
    "                    edge_index=dic_sample['edge_index'],\n",
    "                    root_index=dic_sample['root_index'],\n",
    "                    tree_len=dic_sample['tree_len'],\n",
    "                    status=dic_sample['status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlsELvUtJD3Q"
   },
   "outputs": [],
   "source": [
    "len(total_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7OAeH2JXk1b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class Node_tweet:\n",
    "    def __init__(self, idx=None):\n",
    "        self.children = []\n",
    "        self.idx = idx\n",
    "        self.word = []\n",
    "        self.index = []\n",
    "        self.parent = None\n",
    "\n",
    "def str2matrix(Str):\n",
    "    wordFreq, wordIndex = [], []\n",
    "    for pair in Str.split(' '):\n",
    "        freq, index = map(float, pair.split(':'))\n",
    "        if index <= 5000:\n",
    "            wordFreq.append(freq)\n",
    "            wordIndex.append(int(index))\n",
    "    return wordFreq, wordIndex\n",
    "\n",
    "def constructMat(tree):\n",
    "    index2node = {}\n",
    "    for i in tree:\n",
    "        node = Node_tweet(idx=i)\n",
    "        index2node[i] = node\n",
    "    for j in tree:\n",
    "        indexC = j\n",
    "        indexP = tree[j]['parent']\n",
    "        nodeC = index2node[indexC]\n",
    "        wordFreq, wordIndex = str2matrix(tree[j]['vec'])\n",
    "        nodeC.index = wordIndex\n",
    "        nodeC.word = wordFreq\n",
    "        if indexP != 'None':\n",
    "            nodeP = index2node[int(indexP)]\n",
    "            nodeC.parent = nodeP\n",
    "            nodeP.children.append(nodeC)\n",
    "        else:\n",
    "            root_index = nodeC.index\n",
    "            root_word = nodeC.word\n",
    "    rootfeat = np.zeros([1, 5000])\n",
    "    if root_index:\n",
    "        rootfeat[0, np.array(root_index)] = np.array(root_word)\n",
    "    matrix = np.zeros([len(index2node), len(index2node)])\n",
    "    row, col = [], []\n",
    "    x_word, x_index = [], []\n",
    "    for index_i, node_i in enumerate(index2node.values()):\n",
    "        for index_j, node_j in enumerate(index2node.values()):\n",
    "            if node_i.children and node_j in node_i.children:\n",
    "                matrix[index_i][index_j] = 1\n",
    "                row.append(index_i)\n",
    "                col.append(index_j)\n",
    "        x_word.append(node_i.word)\n",
    "        x_index.append(node_i.index)\n",
    "    edgematrix = [row, col]\n",
    "    return x_word, x_index, edgematrix, rootfeat\n",
    "\n",
    "def getfeature(x_word, x_index):\n",
    "    x = np.zeros([len(x_index), 5000])\n",
    "    for i, index in enumerate(x_index):\n",
    "        if index:\n",
    "            x[i, np.array(index)] = np.array(x_word[i])\n",
    "    return x\n",
    "\n",
    "def loadEid(tree, id, label):\n",
    "    if tree is None or len(tree) < 2:\n",
    "        return None\n",
    "    x_word, x_index, tree, rootfeat = constructMat(tree)\n",
    "    x_x = getfeature(x_word, x_index)\n",
    "    rootfeat, tree, x_x = np.array(rootfeat), np.array(tree), np.array(x_x)\n",
    "    np.savez(f'/content/drive/MyDrive/ProjectData/Interaction_graph/{id}.npz', x=x_x, root=rootfeat, edgeindex=tree, y=label)\n",
    "\n",
    "def main(obj):\n",
    "    treePath = f'/content/drive/MyDrive/ProjectData/data.TD_RvNN.vol_5000.txt'\n",
    "    labelPath = f'/content/drive/MyDrive/ProjectData/{obj}_label_All.txt'\n",
    "\n",
    "    treeDic = {}\n",
    "    for line in open(treePath):\n",
    "        eid, indexP, indexC, max_degree, maxL, Vec = line.rstrip().split('\\t')\n",
    "        if eid not in treeDic:\n",
    "            treeDic[eid] = {}\n",
    "        treeDic[eid][int(indexC)] = {'parent': indexP, 'max_degree': int(max_degree), 'maxL': int(maxL), 'vec': Vec}\n",
    "\n",
    "    event, labels = [], {}\n",
    "    labelset_nonR, labelset_f, labelset_t, labelset_u = ['news', 'non-rumor'], ['false'], ['true'], ['unverified']\n",
    "    for line in open(labelPath):\n",
    "        label, eid = line.rstrip().split('\\t')[0], line.rstrip().split('\\t')[2].lower()\n",
    "        event.append(eid)\n",
    "        if label in labelset_nonR:\n",
    "            labels[eid] = 0\n",
    "        elif label in labelset_f:\n",
    "            labels[eid] = 1\n",
    "        elif label in labelset_t:\n",
    "            labels[eid] = 2\n",
    "        elif label in labelset_u:\n",
    "            labels[eid] = 3\n",
    "\n",
    "    Parallel(n_jobs=-1, backend='threading')(delayed(loadEid)(treeDic[eid], eid, labels[eid]) for eid in tqdm(event))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    obj = 'Twitter15'  # Update 'Your_Object_Name' with the specific dataset name\n",
    "    main(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JLsEON9efas"
   },
   "outputs": [],
   "source": [
    "pip install torch-scatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zNDn1aChUyD"
   },
   "outputs": [],
   "source": [
    "pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QX6k7eBjDNr"
   },
   "outputs": [],
   "source": [
    "pip install torch-spline-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OE05ZS0si2Pu"
   },
   "outputs": [],
   "source": [
    "pip install torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L76_PIY_iy0A"
   },
   "outputs": [],
   "source": [
    "pip install torch-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3AakKMPtKBt"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade torch torchvision torchaudio torchtext torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://pytorch-geometric.com/whl/torch-$(echo $TORCH_VERSION | cut -f1-3 -d'.').html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3iWR_TrujHT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e76kxCmUx4OD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def root_edge_enhance(row, col):\n",
    "    c = set(row).union(set(col))\n",
    "    sorted_list = sorted(c)\n",
    "\n",
    "    if sorted_list[0] != 0:\n",
    "        return row, col\n",
    "\n",
    "    new_row = []\n",
    "    new_col = []\n",
    "    for element in sorted_list[1:]:\n",
    "        new_row.append(0)\n",
    "        new_col.append(element)\n",
    "\n",
    "    indices_row = [index for index, value in enumerate(row) if value == 0]\n",
    "    row = [row[i] for i in range(len(row)) if i not in indices_row]\n",
    "    col = [col[i] for i in range(len(col)) if i not in indices_row]\n",
    "\n",
    "    indices_col = [index for index, value in enumerate(col) if value == 0]\n",
    "    row = [row[i] for i in range(len(row)) if i not in indices_col]\n",
    "    col = [col[i] for i in range(len(col)) if i not in indices_col]\n",
    "\n",
    "    row.extend(new_row)\n",
    "    col.extend(new_col)\n",
    "    return row, col\n",
    "\n",
    "class UdGraphDataset(Dataset):\n",
    "    def __init__(self, fold_x, treeDic, dataname, lower=2, upper=100000, droprate=0):\n",
    "        self.fold_x = list(filter(lambda id: id in treeDic and len(treeDic[id]) >= lower and len(treeDic[id]) <= upper, fold_x))\n",
    "        self.treeDic = treeDic\n",
    "        self.graph_path = '/content/drive/MyDrive/ProjectData/' + dataname + 'Interaction_graph/'\n",
    "        self.ego_path = '/content/drive/MyDrive/ProjectData/' + dataname + 'Ego_graph/'\n",
    "        self.droprate = droprate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fold_x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.fold_x[index]\n",
    "\n",
    "        # Load graph data\n",
    "        data = np.load(self.graph_path + str(id) + '.npz', allow_pickle=True)\n",
    "        edgeindex = data['edgeindex']\n",
    "\n",
    "        # Enhance edge connections\n",
    "        row, col = root_edge_enhance(list(edgeindex[0]), list(edgeindex[1]))\n",
    "        burow = list(col)\n",
    "        bucol = list(row)\n",
    "        row.extend(burow)\n",
    "        col.extend(bucol)\n",
    "\n",
    "        # Apply droprate if required\n",
    "        if self.droprate > 0:\n",
    "            length = len(row)\n",
    "            poslist = np.random.choice(length, int(length * (1 - self.droprate)), replace=False)\n",
    "            row = [row[i] for i in poslist]\n",
    "            col = [col[i] for i in poslist]\n",
    "\n",
    "        new_edgeindex = [row, col]\n",
    "\n",
    "        # Load ego-centric network data\n",
    "        user_ego = np.load(self.ego_path + str(id) + '.npz', allow_pickle=True)\n",
    "        ego_twitter_id = id\n",
    "        ego_root_feature = np.array(eval(str(user_ego['root_feature'])))\n",
    "        ego_tree_feature = np.array(eval(str(user_ego['tree_feature'])))\n",
    "        ego_edge_index = np.array(eval(str(user_ego['edge_index'])))\n",
    "        ego_root_index = user_ego['root_index']\n",
    "\n",
    "        # Prepare and return PyTorch geometric Data objects\n",
    "        return (\n",
    "            Data(\n",
    "                x=torch.tensor(data['x'], dtype=torch.float32),\n",
    "                edge_index=torch.LongTensor(new_edgeindex),\n",
    "                y=torch.LongTensor([int(data['y'])]),\n",
    "                root=torch.LongTensor(data['root']),\n",
    "                rootindex=torch.LongTensor([int(data['rootindex'])])\n",
    "            ),\n",
    "            Data(\n",
    "                x=torch.tensor(ego_tree_feature, dtype=torch.float32),\n",
    "                edge_index=torch.LongTensor(ego_edge_index),\n",
    "                y=torch.LongTensor([int(data['y'])]),\n",
    "                root=torch.LongTensor([ego_root_feature]),\n",
    "                rootindex=torch.LongTensor([int(ego_root_index)]),\n",
    "                tree_text_id=torch.LongTensor([int(ego_twitter_id)])\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyFhGd1hdqjd"
   },
   "outputs": [],
   "source": [
    "def loadUdData(dataname, treeDic, fold_x_train, fold_x_test, droprate):\n",
    "    print(\"Loading train set...\")\n",
    "    traindata_list = UdGraphDataset(fold_x_train, treeDic, dataname=dataname, droprate=droprate)\n",
    "    print(\"Number of samples in the train set:\", len(traindata_list))\n",
    "\n",
    "    print(\"\\nLoading test set...\")\n",
    "    testdata_list = UdGraphDataset(fold_x_test, treeDic, dataname=dataname, droprate=droprate)\n",
    "    print(\"Number of samples in the test set:\", len(testdata_list))\n",
    "\n",
    "    return traindata_list, testdata_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBmYo7FMesS-"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.accs = 0\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, accs, model, modelname, str):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.accs = accs\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            # print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.accs = accs\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, modelname, str):\n",
    "        if val_loss < self.val_loss_min:\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model...')\n",
    "            torch.save(model.state_dict(), modelname + str + '.m')\n",
    "            self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qdqtj0nfdMw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, init_size, hidden_size, output_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.init_size = init_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.line_q = nn.Linear(self.init_size, self.hidden_size, bias=False)\n",
    "        self.line_k = nn.Linear(self.init_size, self.hidden_size, bias=False)\n",
    "        self.line_v = nn.Linear(self.init_size, self.output_size, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        query = self.line_q(query)\n",
    "        key = self.line_k(key)\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = F.dropout(p_attn, p=dropout, training=self.training)\n",
    "\n",
    "        return self.line_v(torch.matmul(p_attn, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqBKdNnLnJbA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import GCNConv\n",
    "import copy\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, init_size, hidden_size, output_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.init_size = init_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.line_q = nn.Linear(self.init_size, self.hidden_size, bias=False)\n",
    "        self.line_k = nn.Linear(self.init_size, self.hidden_size, bias=False)\n",
    "        self.line_v = nn.Linear(self.init_size, self.output_size, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        query = self.line_q(query)\n",
    "        key = self.line_k(key)\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k).to(query.device))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = F.dropout(p_attn, p=dropout, training=self.training)\n",
    "\n",
    "        return self.line_v(torch.matmul(p_attn, value))\n",
    "\n",
    "\n",
    "class EgoEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super(EgoEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hid_feats)\n",
    "        self.conv2 = GCNConv(hid_feats + in_feats, out_feats)\n",
    "        self.dropout_rate = 0.2\n",
    "        self.w1 = nn.Linear(hid_feats * 3, hid_feats * 3)\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x1 = copy.copy(x.float())\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x2 = copy.copy(x)\n",
    "        rootindex = data.rootindex\n",
    "        root_extend = torch.zeros(len(data.batch), x1.size(1)).to(x.device)\n",
    "        batch_size = max(data.batch) + 1\n",
    "        for num_batch in range(batch_size):\n",
    "            index = (data.batch == num_batch)\n",
    "            root_extend[index] = x1[rootindex[num_batch]]\n",
    "        x = torch.cat((x, root_extend), 1)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        root_extend = torch.zeros(len(data.batch), x2.size(1)).to(x.device)\n",
    "        for num_batch in range(batch_size):\n",
    "            index = (data.batch == num_batch)\n",
    "            root_extend[index] = x2[rootindex[num_batch]]\n",
    "        x = torch.cat((x, root_extend, x2), 1)\n",
    "        x = scatter_mean(F.relu(self.w1(x)), data.batch, dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InteractionEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super(InteractionEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_feats, hid_feats)\n",
    "        self.conv2 = GCNConv(hid_feats + in_feats, out_feats)\n",
    "        self.w1 = nn.Linear(hid_feats * 3, hid_feats * 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x1 = copy.copy(x.float())\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x2 = copy.copy(x)\n",
    "        rootindex = data.rootindex\n",
    "        root_extend = torch.zeros(len(data.batch), x1.size(1)).to(x.device)\n",
    "        batch_size = max(data.batch) + 1\n",
    "        for num_batch in range(batch_size):\n",
    "            index = (data.batch == num_batch)\n",
    "            root_extend[index] = x1[rootindex[num_batch]]\n",
    "        x = torch.cat((x, root_extend), 1)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        root_extend = torch.zeros(len(data.batch), x2.size(1)).to(x.device)\n",
    "        for num_batch in range(batch_size):\n",
    "            index = (data.batch == num_batch)\n",
    "            root_extend[index] = x2[rootindex[num_batch]]\n",
    "        x = torch.cat((x, root_extend, x2), 1)\n",
    "        x = scatter_mean(F.relu(self.w1(x)), data.batch, dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RDMSC(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, atten_out_dim):\n",
    "        super(RDMSC, self).__init__()\n",
    "        self.EgoEncoder = EgoEncoder(10, hid_feats, out_feats)\n",
    "        self.InteractionEncoder = InteractionEncoder(in_feats, hid_feats, out_feats)\n",
    "        self.Atten = Attention(hid_feats * 3, hid_feats * 3, atten_out_dim)\n",
    "        self.fc = nn.Linear(hid_feats * 6 + atten_out_dim, 4)\n",
    "\n",
    "    def forward(self, data, data2):\n",
    "        EE_x = self.EgoEncoder(data2)\n",
    "        IE_x = self.InteractionEncoder(data)\n",
    "        query = copy.deepcopy(IE_x.detach())\n",
    "        key = value = copy.deepcopy(EE_x.detach())\n",
    "        Attn = self.Atten(query=query, key=key, value=value, dropout=0.5)\n",
    "        x = torch.cat((IE_x, EE_x, Attn),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtHpnGE5rvqq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
